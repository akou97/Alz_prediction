# -*- coding: utf-8 -*-
"""Projet_Long_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JAwvGBIEjrQ9KSsccyDfWzdHGGVCRO9k

#Extraction des features
"""

import os
import numpy as np
from zipfile import ZipFile
import matplotlib.pyplot as plt
import seaborn as sns

from keras.models import Model
from keras.applications import VGG16
from keras.applications.vgg16  import preprocess_input
from keras.preprocessing.image import load_img, img_to_array

#Parameters

#var = 'raw'
var = "pressure"
#var = "penups_raw"

#les données
zipo = ZipFile(var+"_augmented.zip")
zipo.extractall()

path_alz =  'augmented/alz/'
path_control = 'augmented/control/'
n_alz = len(os.listdir(path_alz))
n_control = len(os.listdir(path_alz))

labels=['alz', 'control']

#Exemple
#affichage_image(path_alz, 4)

base_model = VGG16(weights='imagenet')
#base_model.summary()

"""**Extract features**"""

def compute_features(model, img_path, rescale = True ,name='VGG'):
  
  if name == 'Xception':
    img = load_img(img_path, target_size = (299,299,3))
  else:
    img = load_img(img_path, target_size = (224,224,3))
  x = img_to_array(img)
  if rescale:
    x = x/255
  x = np.expand_dims(x, axis=0)
  x = preprocess_input(x)
  features = model.predict(x)
  return features

couche = 'flatten'

print('Extraction Feature from :'+couche)
model = Model(inputs = base_model.input, outputs = base_model.get_layer(couche).output)
Features = list()

for file_path in os.listdir(path_alz):
  file_path = path_alz+file_path
  feat = compute_features(model, file_path, rescale=True)[0]
  Features.append(feat.reshape(-1))

for file_path in os.listdir(path_control):
  file_path = path_control+file_path
  feat = compute_features(model, file_path, rescale=True)[0]
  Features.append(feat.reshape(-1))
  
#transform to a numpy array 
Features = np.array(Features)
print("the shape of your features matrix is :",Features.shape)
np.savez_compressed(var+'_'+couche+'.npz', Features)

#Our target
target = np.zeros(len(Features))
target[:n_alz] = np.ones(n_alz)

print("the shape of your features matrix is :",Features.shape)

np.savez_compressed(var+'.npz', Features)
print("Save the features : Done !")

"""# Visualisation des features maps"""

import tensorflow as tf
import numpy as np
import os

from keras.models import Model
import matplotlib.pyplot as plt
from numpy import expand_dims

from keras.applications.vgg16 import preprocess_input, VGG16
from keras.preprocessing.image import load_img, img_to_array

from time import time

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split, GridSearchCV
import seaborn as sns

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

from zipfile import ZipFile, ZIP_DEFLATED

#var = 'raw'
var = 'pressure'


zipo = ZipFile(var+'_augmented.zip')
zipo.extractall()

!mkdir 'featureMap/'

image_size = 224
fichier = "A_0009_0.png" 
image_path = "augmented/alz/"+fichier
#image_path = "augmented/alz/A_0005_0.png"

# load the model
model = VGG16()
# redefine model to output right after the first hidden layer
ixs = [2, 5, 9, 13, 17]
filter = [40, 110, 50, 400 , 506]

outputs = [model.layers[i].output for i in ixs]
layer_name =  [model.layers[i]._name for i in ixs]
model = Model(inputs=model.inputs, outputs=outputs)
# load the image with the required shape
img = load_img(image_path, target_size=(image_size, image_size))
# convert the image to an array
img = img_to_array(img)
# expand dimensions so that it represents a single 'sample'
img = expand_dims(img, axis=0)
# prepare the image (e.g. scale pixel values for the vgg)
img = preprocess_input(img)
# get feature map for first hidden layer
feature_maps = model.predict(img)
# plot the output from each block
square = 1
cpt = 0
for fmap in feature_maps:
  # plot all 64 maps in an 8x8 squares
  ix = filter[cpt]
  plt.figure(figsize=(2,2))
  for _ in range(square):
    for _ in range(square):
      # specify subplot and turn of axis
      ax = plt.subplot(square, square, 1)

      ax.set_xticks([])
      ax.set_yticks([])
      # plot filter channel in grayscale
      plt.imshow(fmap[0, :, :, ix], cmap='gray')
      ix += 1
  # show the figure
  plt.suptitle(layer_name[cpt]+' ('+str(ix)+')')
  plt.savefig('featureMap/'+var+'_'+layer_name[cpt]+'_'+fichier+'.png',format="PNG")
  plt.show()
  
  cpt+=1

break

layer_name = [model.layers[i]._name for i in ixs]
#layer_name

def zipdir(path, ziph):
    # ziph is zipfile handle
    for root, dirs, files in os.walk(path):
        for file in files:
            ziph.write(os.path.join(root, file), 
                       os.path.relpath(os.path.join(root, file), 
                                       os.path.join(path, '..')))
      
zipf = ZipFile('featureMap.zip', 'w', ZIP_DEFLATED)
zipdir('featureMap/', zipf)
zipf.close()

#model.summary()

"""# PCA Decomposition"""

#load features

var = 'pressure'
#var = 'raw'
#var = "penups_raw"
feature = np.load(var +'_feat.npz')['arr_0']

#feature = np.load(var +'_feat.npz')['arr_0']

n_alz = len(feature)//2
n_control = len(feature)//2

#Our target
target = np.zeros(len(feature))
target[:n_alz] = np.ones(n_alz)

labels = ['AD', 'HC']

def plot_pca(F_train, y_train,F_test, y_test, target_name, var ,
             ratio = 0.95, colors = ['navy', 'turquoise']):
  
  n_example, n_feat = F_train.shape

  acp = PCA(n_components=n_example)
  acp.fit(F_train)
  X_train = acp.transform(F_train)
  X_test = acp.transform(F_test)

  plt.figure(figsize=(10,4))
  plt.title("PCA decomposition of " + var)
  for i in [0, 1]:
    plt.scatter(X_train[y_train == i, 0], X_train[y_train == i, 1], color=colors[i],
                  lw=2, label = target_name[i])
    plt.xlabel('Axis 0 - {:2.2f} %'.format(acp.explained_variance_[0]/np.cumsum(acp.explained_variance_)[-1]*100))
    plt.ylabel('Axis 1 - {:2.2f} %'.format(acp.explained_variance_[1]/np.cumsum(acp.explained_variance_)[-1]*100))
    plt.legend()
  
  #save the image
  plt.savefig("pca_"+var+'.png', format="PNG") 
  plt.show()

 #calculate the number of axis minimal of 95% inertia
  cpt = 0
  for e in np.cumsum(acp.explained_variance_):
    if e > ratio*np.cumsum(acp.explained_variance_)[-1]:
      break
    cpt+=1
  print("Nombre d'axe nécessaire pour représenter ",ratio*100,"% d'information est "
  ,cpt)

  plt.figure(figsize=(10,4))
  plt.title("With {:} features we represent {:.1f} % of inertia ".format(cpt, ratio*100))
  plt.plot(range(n_example),np.cumsum(acp.explained_variance_)/np.sum(acp.explained_variance_)*100, '*-' )
  plt.ylabel('Inertia intra-classe')
  plt.xlabel('index of axis')
  plt.xlim(left=0, right=70)
  #save the image
  plt.savefig("inertia_"+var+'.png', format="PNG") 
  
  return X_train[:,:cpt],X_test[:,:cpt]

"""#Classification"""

X = feature
y = target 
indice = np.arange(len(feature))

f_train, f_test, y_train, y_test, idx_train, idx_test = train_test_split( X, y,indice,
                                                                         test_size=0.333,
                                                                         random_state=42, stratify = y)

y_test.shape

X_train, X_test = plot_pca(f_train, y_train,f_test, y_test, labels, var )

print('the shape of X_train is',X_train.shape)
print('the shape of X_test is',X_test.shape)

def sensibility(clf, X, y):
  y_pred = clf.predict(X)
  cm = confusion_matrix(y, y_pred)
  tn = cm[0, 0]
  fp = cm[0, 1] 
  fn = cm[1, 0] 
  tp = cm[1, 1]

  return tp/(tp+fn)

def specificity(clf, X, y):
  y_pred = clf.predict(X)
  cm = confusion_matrix(y, y_pred)
  tn = cm[0, 0]
  fp = cm[0, 1] 
  fn = cm[1, 0] 
  tp = cm[1, 1]

  return tn/(tn+fp)

#SVM
print("Fitting the classifier to the training set")
t0 = time()
param_grid = {'C': [10, 1e2 ,1e3, 5e3,1e4 ],#5e3, 1e4, 5e4, 1e5
              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01],
              'kernel' : ['rbf'],}

scoring = {'AUC': 'roc_auc', 'score': 'accuracy',
           'sensib' : sensibility, 'spec' : specificity}
clf1 = GridSearchCV(
    SVC(class_weight='balanced'), param_grid,
    cv=10, scoring=scoring, refit='AUC', return_train_score=True
)
clf1 = clf1.fit(X_train, y_train)
print("done in %0.3fs" % (time() - t0))
print("Best estimator found by grid search:")
print(clf1.best_estimator_)

t0 = time()
y_pred = clf1.predict(X_test)
print("done in %0.3fs" % (time() - t0))

print(classification_report(y_test, y_pred, target_names=labels))
cm = confusion_matrix(y_test, y_pred, labels=range(2))

sns.heatmap(cm, annot=True, fmt='d')

cpt = 4
print('accuracy test %.2f  +- %.2f' %(clf1.cv_results_['mean_test_score'][cpt],
                                      clf1.cv_results_['std_test_score'][cpt]*2))
print('specificity test %.2f  +- %.2f' %(clf1.cv_results_['mean_test_spec'][cpt],
                                      clf1.cv_results_['std_test_spec'][cpt]*2))
print('sensibility test %.2f  +- %.2f' %(clf1.cv_results_['mean_test_sensib'][cpt],
                                      clf1.cv_results_['std_test_sensib'][cpt]*2))
print('AUC test %.2f  +- %.2f' %(clf1.cv_results_['mean_test_AUC'][cpt],
                                      clf1.cv_results_['std_test_AUC'][cpt]*2))

# Commented out IPython magic to ensure Python compatibility.
means = clf1.cv_results_['mean_test_score']
stds = clf1.cv_results_['std_test_score']
i = 0
for mean, std, params in zip(means, stds, clf1.cv_results_['params']):
    print("%d  %0.3f (+/-%0.03f) for %r"
#           % (i,mean, std * 2, params))
    i+=1
print()

!mkdir "missclassif"
!mkdir "missclassif/"+var
!mkdir "missclassif/"+var+"/alz"
!mkdir "missclassif/"+var+"/control"

mis_classified = idx_test[y_pred!=y_test]
mis_classified_alz = mis_classified[mis_classified<n_alz]
mis_classified_control = mis_classified[mis_classified>=n_alz]
mis_classified_control

square = 4
ix = 1

for id in mis_classified_control[:square]:
  # plot all 64 maps in an 8x8 squares
  #ix = filter[cpt]
  name_image = os.listdir('augmented/control/')[id-n_alz]
  plt.figure(figsize=(5,5))
  # specify subplot and turn of axis
  ax = plt.subplot(1, 1, 1)

  ax.set_xticks([])
  ax.set_yticks([])
  # plot filter channel in grayscale
  img = load_img('augmented/control/'+name_image)
  plt.title(name_image)
  plt.imshow(img)

  ix += 1
  # show the figure
  
  plt.savefig("missclassif/"+var+"/control/"+name_image,format="PNG")
  plt.show()
  
  #cpt+=1

square = 4
ix = 1
for id in mis_classified_alz[:square]:
  # plot all 64 maps in an 8x8 squares
  #ix = filter[cpt]
  name_image = os.listdir('augmented/alz/')[id-n_alz]
  plt.figure(figsize=(5,5))

  # specify subplot and turn of axis
  ax = plt.subplot(1, 1, 1)

  ax.set_xticks([])
  ax.set_yticks([])
  # plot filter channel in grayscale
  img = load_img('augmented/alz/'+name_image)
  plt.title(name_image)
  plt.imshow(img)

  ix += 1
  # show the figure
  
  #plt.savefig('featureMap/'+var+'_'+layer_name[cpt]+'_'+fichier+'.png',format="PNG")
  plt.savefig("missclassif/"+var+"/alz/"+name_image,format="PNG")
  plt.show()
  
  #cpt+=1

#MLP classifier
print("Fitting the classifier to the training set")
t0 = time()

parameters = {'solver': ['lbfgs'], 'max_iter': [5000, 15000],
              'alpha': 10.0 ** -np.arange(1, 5), 
              'hidden_layer_sizes':(np.arange(10, 15)) 
              }

scoring = {'AUC': 'roc_auc', 'score': 'accuracy',
           'sensib' : sensibility, 'spec' : specificity}

clf = GridSearchCV(MLPClassifier(), parameters,cv=10, scoring=scoring, 
                   refit='AUC', return_train_score=True)
clf = clf.fit(X_train, y_train)
print("done in %0.3fs" % (time() - t0))
print("Best estimator found by grid search:")
print(clf.best_estimator_)

t0 = time()
y_pred = clf.predict(X_test)
print("done in %0.3fs" % (time() - t0))

print(classification_report(y_test, y_pred, target_names=labels))
cm = confusion_matrix(y_test, y_pred, labels=range(2))

sns.heatmap(cm, annot=True, fmt='d')

cpt = 0
print('accuracy test %.2f  +- %.2f' %(clf.cv_results_['mean_test_score'][cpt],
                                      clf.cv_results_['std_test_score'][cpt]*2))
print('specificity test %.2f  +- %.2f' %(clf.cv_results_['mean_test_spec'][cpt],
                                      clf.cv_results_['std_test_spec'][cpt]*2))
print('sensibility test %.2f  +- %.2f' %(clf.cv_results_['mean_test_sensib'][cpt],
                                      clf.cv_results_['std_test_sensib'][cpt]*2))
print('AUC test %.2f  +- %.2f' %(clf.cv_results_['mean_test_AUC'][cpt],
                                      clf.cv_results_['std_test_AUC'][cpt]*2))

# Commented out IPython magic to ensure Python compatibility.
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
i = 0
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print("%d  %0.3f (+/-%0.03f) for %r"
#           % (i,mean, std * 2, params))
    i+=1
print()

break

#KNN classifier
print("Fitting the classifier to the training set")
t0 = time()

parameters = { 'weights': ["uniform", "distance"],
              "n_neighbors" : (np.arange(3,8)) , 'algorithm' : ["ball_tree", "kd_tree", "brute"] 
              }
scoring = {'AUC': 'roc_auc', 'score': 'accuracy',
           'sensib' : sensibility, 'spec' : specificity}

clf = GridSearchCV(KNeighborsClassifier(), parameters,  cv=10,
                   scoring=scoring, 
                   refit='AUC', return_train_score=True)
clf = clf.fit(X_train, y_train)
print("done in %0.3fs" % (time() - t0))
print("Best estimator found by grid search:")
print(clf.best_estimator_)

t0 = time()
y_pred = clf.predict(X_test)
print("done in %0.3fs" % (time() - t0))

print(classification_report(y_test, y_pred, target_names=labels))
cm = confusion_matrix(y_test, y_pred, labels=range(2))

sns.heatmap(cm, annot=True, fmt='d')

# Commented out IPython magic to ensure Python compatibility.
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
i = 0
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print("%d  %0.3f (+/-%0.03f) for %r"
#           % (i,mean, std * 2, params))
    i+=1
print()

cpt = 1
print('accuracy test %.2f  +- %.2f' %(clf.cv_results_['mean_test_score'][cpt],
                                      clf.cv_results_['std_test_score'][cpt]*2))
print('specificity test %.2f  +- %.2f' %(clf.cv_results_['mean_test_spec'][cpt],
                                      clf.cv_results_['std_test_spec'][cpt]*2))
print('sensibility test %.2f  +- %.2f' %(clf.cv_results_['mean_test_sensib'][cpt],
                                      clf.cv_results_['std_test_sensib'][cpt]*2))
print('AUC test %.2f  +- %.2f' %(clf.cv_results_['mean_test_AUC'][cpt],
                                      clf.cv_results_['std_test_AUC'][cpt]*2))